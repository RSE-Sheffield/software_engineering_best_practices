{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Testing is extremely important. Without testing, you cannot be sure that your code is doing what you think. Testing is an integral part of software development, and should be done *while* you are writing code, not after the code has been written.\n",
    "\n",
    "No doubt sofar, you have been manually checking that your code does the right thing. Perhaps you are tunning your code over a particular input file and making sure that you get a correct-looking plot out at the end. This is a start but how can you be sure that there's not a subtle bug that means that the output is incorrect? And if there *is* a problem, how will you be able to work out exactly which line of code it causing it?\n",
    "\n",
    "In order to be confident that our code it giving a correct output, a *test suite* is useful which provides a set of known inputs and checks that the code matches a set of known, expected outputs. To make it easier to locate where a bug is occuring, it's a good idea to make each individual test run over as small an amount of code as possible so that if *that* test fails, you know where to look for the problem. In Python this \"small unit of code\" is usually a function.\n",
    "\n",
    "Let's get started by making sure that our `add_arrays` function matches the outputs we expect.\n",
    "\n",
    "Since we are writing some tests for our `arrays` module, let's make a file called `test_arrays.py` which contains the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    if output == expect:\n",
    "        print(\"OK\")\n",
    "    else:\n",
    "        print(\"BROKEN\")\n",
    "\n",
    "test_add_arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script defines a function called `test_add_arrays` which defines some known input (`a` and `b`) and a known, matching output (`expect`). It passes them to the function `add_arrays` and compares the output to `expected`. It will either print `OK` or `BROKEN` depending on whether it's working or not. Finally, we explicitly call the test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# TODO Make this appear to the reader as `python test_arrays.py` and hide the \"In []\"\n",
    "\n",
    "%run test_arrays.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Break the test by changing either `a`, `b` or `expected` and rerun the test script. Make sure that it prints `BROKEN` in this case. Change it back to a working state once you've done this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... Change it to use an assert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect\n",
    "\n",
    "test_add_arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..now when we run we get nothing printed on success:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run test_arrays.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but on a failure we get an error printed like:\n",
    "\n",
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"test_arrays.py\", line 13, in <module>\n",
    "    test_add_arrays()\n",
    "  File \"test_arrays.py\", line 11, in test_add_arrays\n",
    "    assert output == expect\n",
    "AssertionError\n",
    "```\n",
    "\n",
    "which, if we had many test functions being run would tell us which one failed and on which line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...We want to automate the runnng of these tests and there is a tool called `pytest` which does this.\n",
    "\n",
    "Remove the call to `test_add_arrays()` on the last line of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run `pytest`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: COLU!COLUMNS=80: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!COLU!COLUMNS=80 venv/bin/pytestMNS=80 venv/bin/pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest will automatically...\n",
    "\n",
    "...If we break the test then we see...\n",
    "\n",
    "\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 999]  # Changed this to break the test\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0\n",
      "rootdir: /home/matt/projects/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_arrays.py \u001b[31mF\u001b[0m\u001b[36m                                                         [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_add_arrays ________________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_add_arrays():\u001b[0m\n",
      "\u001b[1m        a = [1, 2, 3]\u001b[0m\n",
      "\u001b[1m        b = [4, 5, 6]\u001b[0m\n",
      "\u001b[1m        expect = [5, 7, 999]  # Changed this to break the test\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        output = add_arrays(a, b)\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m>       assert output == expect\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert [5, 7, 9] == [5, 7, 999]\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 2 diff: 9 != 999\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get the full diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_arrays.py\u001b[0m:11: AssertionError\n",
      "\u001b[31m\u001b[1m============================== 1 failed in 0.16s ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=80 venv/bin/pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from this is better than we saw with the `assert`. It's printing the full context of the contents of the test function with the line where the `assert` is failing being marked with a `>`. It then gives an expanded explanation of why the assert failed. Before we just got `AssertionError` but now it printsout the contents of `output` and `expect` and tells us that at index 2 of the list it's finding a `9` where we told it to expect a `999`.\n",
    "\n",
    "...Make sure to change it back so that the test passes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]  # Changed this back to 9\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Write a test which tests your `subtract_arrays` function from the previous chapter. Make sure it passes with a correct input/output and correctly fails if you break it on purpose. [<small>answer</small>](answer_subtract_test.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid repeating ourselves\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays1():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect\n",
    "\n",
    "def test_add_arrays2():\n",
    "    a = [-1, -5, -3]\n",
    "    b = [-4, -3, 0]\n",
    "    expect = [-5, -8, -3]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/projects/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "test_arrays.py::test_add_arrays1 \u001b[32mPASSED\u001b[0m\u001b[36m                                  [ 50%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays2 \u001b[32mPASSED\u001b[0m\u001b[36m                                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m============================== 2 passed in 0.04s ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=80 venv/bin/pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "@pytest.mark.parametrize(\"a,b,expect\", [\n",
    "    ([1, 2, 3],    [4, 5, 6],   [5, 7, 9]),\n",
    "    ([-1, -5, -3], [-4, -3, 0], [-5, -8, -3]),\n",
    "])\n",
    "def test_add_arrays(a, b, expect):\n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/projects/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "test_arrays.py::test_add_arrays[a0-b0-expect0] \u001b[32mPASSED\u001b[0m\u001b[36m                    [ 50%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays[a1-b1-expect1] \u001b[32mPASSED\u001b[0m\u001b[36m                    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m============================== 2 passed in 0.04s ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=80 venv/bin/pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "- Add some more parameters to the `test_add_arrays` function.\n",
    "- Parametrise the subtract function."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
