{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Testing is extremely important. Without testing, you cannot be sure that your code is doing what you think. Testing is an integral part of software development, and should be done *while* you are writing code, not after the code has been written.\n",
    "\n",
    "No doubt so far, you have been manually checking that your code does the right thing. Perhaps you are tunning your code over a particular input file and making sure that you get a correct-looking plot out at the end. This is a start but how can you be sure that there's not a subtle bug that means that the output is incorrect? And if there *is* a problem, how will you be able to work out exactly which line of code it causing it?\n",
    "\n",
    "In order to be confident that our code it giving a correct output, a *test suite* is useful which provides a set of known inputs and checks that the code matches a set of known, expected outputs. To make it easier to locate where a bug is occuring, it's a good idea to make each individual test run over as small an amount of code as possible so that if *that* test fails, you know where to look for the problem. In Python this \"small unit of code\" is usually a function.\n",
    "\n",
    "Let's get started by making sure that our `add_arrays` function matches the outputs we expect. As a reminder, this is what the file `arrays.py` looks like (though you will have a second function, `subtract_arrays` in yours):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile arrays.py\n",
    "\n",
    "\"\"\"\n",
    "This module contains functions for manipulating and combining Python lists.\n",
    "\"\"\"\n",
    "\n",
    "def add_arrays(x, y):\n",
    "    \"\"\"\n",
    "    This function adds together each element of the two passed lists.\n",
    "\n",
    "    Args:\n",
    "        x (list): The first list to add\n",
    "        y (list): The second list to add\n",
    "\n",
    "    Returns:\n",
    "        list: the pairwise sums of ``x`` and ``y``.\n",
    "\n",
    "    Examples:\n",
    "        >>> add_arrays([1, 4, 5], [4, 3, 5])\n",
    "        [5, 7, 10]\n",
    "    \"\"\"\n",
    "    z = []\n",
    "    for x_, y_ in zip(x, y):\n",
    "        z.append(x_ + y_)\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the name of the module we want to test is `arrays`, let's make a file called `test_arrays.py` which contains the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    if output == expect:\n",
    "        print(\"OK\")\n",
    "    else:\n",
    "        print(\"BROKEN\")\n",
    "\n",
    "test_add_arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script defines a function called `test_add_arrays` which defines some known input (`a` and `b`) and a known, matching output (`expect`). It passes them to the function `add_arrays` and compares the output to `expected`. It will either print `OK` or `BROKEN` depending on whether it's working or not. Finally, we explicitly call the test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run test_arrays.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Break the test by changing either `a`, `b` or `expected` and rerun the test script. Make sure that it prints `BROKEN` in this case. Change it back to a working state once you've done this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asserting\n",
    "\n",
    "... Change it to use an assert.\n",
    "\n",
    "The method used here works and runs the code correctly but it doesn't give very good output. If we had five tests in our file and three of them were failing we'd see something like:\n",
    "\n",
    "```\n",
    "OK\n",
    "BROKEN\n",
    "OK\n",
    "BROKEN\n",
    "BROKEN\n",
    "```\n",
    "\n",
    "We'd then have to cross-check back to our code to see which tests the `BROKEN`s referred to.\n",
    "\n",
    "To be able to automatically relate the output of the failing test to the place where your test failed, you can use an `assert` statement.\n",
    "\n",
    "An `assert` statement is followed by something which is either truthy of falsy. If it is truthy then nothing happens but if it is falsy then an exception is raised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 5 == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-05598cd61862>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 5 == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this in the test function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect\n",
    "\n",
    "test_add_arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..now when we run we get nothing printed on success:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run test_arrays.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but on a failure we get an error printed like:\n",
    "\n",
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"test_arrays.py\", line 13, in <module>\n",
    "    test_add_arrays()\n",
    "  File \"test_arrays.py\", line 11, in test_add_arrays\n",
    "    assert output == expect\n",
    "AssertionError\n",
    "```\n",
    "\n",
    "which, if we had many test functions being run would tell us which one failed and on which line.\n",
    "\n",
    "...fist failure, wouldn't continue..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytest\n",
    "\n",
    "...We want to automate the runnng of these tests and there is a tool called `pytest` which does this.\n",
    "\n",
    "Remove the call to `test_add_arrays()` on the last line of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run `pytest`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.1, py-1.8.0, pluggy-0.13.0\n",
      "rootdir: /home/matt/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 1 item                                           \u001b[0m\u001b[1m\n",
      "\n",
      "test_arrays.py \u001b[32m.\u001b[0m\u001b[36m                                     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 1 passed in 4.26s =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest will automatically...\n",
    "\n",
    "...naming...\n",
    "\n",
    "...If we break the test then we see...\n",
    "\n",
    "\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 999]  # Changed this to break the test\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.1, py-1.8.0, pluggy-0.13.0\n",
      "rootdir: /home/matt/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 1 item                                           \u001b[0m\n",
      "\n",
      "test_arrays.py \u001b[31mF\u001b[0m\u001b[36m                                     [100%]\u001b[0m\n",
      "\n",
      "========================= FAILURES =========================\n",
      "\u001b[31m\u001b[1m_____________________ test_add_arrays ______________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_add_arrays():\u001b[0m\n",
      "\u001b[1m        a = [1, 2, 3]\u001b[0m\n",
      "\u001b[1m        b = [4, 5, 6]\u001b[0m\n",
      "\u001b[1m        expect = [5, 7, 999]  # Changed this to break the test\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        output = add_arrays(a, b)\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m>       assert output == expect\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert [5, 7, 9] == [5, 7, 999]\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 2 diff: 9 != 999\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get the full diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_arrays.py\u001b[0m:11: AssertionError\n",
      "\u001b[31m\u001b[1m==================== 1 failed in 0.52s =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from this is better than we saw with the `assert`. It's printing the full context of the contents of the test function with the line where the `assert` is failing being marked with a `>`. It then gives an expanded explanation of why the assert failed. Before we just got `AssertionError` but now it printsout the contents of `output` and `expect` and tells us that at index 2 of the list it's finding a `9` where we told it to expect a `999`.\n",
    "\n",
    "...Make sure to change it back so that the test passes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]  # Changed this back to 9\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Write a test which tests your `subtract_arrays` function from the previous chapter. Make sure it passes with a correct input/output and correctly fails if you break it on purpose. [<small>answer</small>](answer_subtract_test.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid repeating ourselves\n",
    "\n",
    "..lets add a second test to check a different set of inputs and outputs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays1():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect\n",
    "\n",
    "def test_add_arrays2():\n",
    "    a = [-1, -5, -3]\n",
    "    b = [-4, -3, 0]\n",
    "    expect = [-5, -8, -3]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 -- /home/matt/courses/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 2 items                                          \u001b[0m\n",
      "\n",
      "test_arrays.py::test_add_arrays1 \u001b[32mPASSED\u001b[0m\u001b[36m              [ 50%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays2 \u001b[32mPASSED\u001b[0m\u001b[36m              [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 2 passed in 0.25s =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will work well but we've had to repeat ourselves almost entirely. The only difference between the two functions is the inputs and outputs of the function under test. Usually in this case in a function you would take these things as arguments and we can do the same thing here.\n",
    "\n",
    "The actual logic of the function is the following:\n",
    "\n",
    "```python\n",
    "def test_add_arrays(a, b, expect):\n",
    "    output = add_arrays(a, b)\n",
    "    assert output == expect\n",
    "```\n",
    "\n",
    "We then just need a way of passing the data we want to check into this function. For this, pytest provides a feature called *parametrisation*. We label our fucntion with a *decoration* which allows pytest to run it mutliple times with different data.\n",
    "\n",
    "To use this feature we must import the `pytest` module and use the `pytest.mark.parametrize` decorator like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "@pytest.mark.parametrize(\"a,b,expect\", [\n",
    "    ([1, 2, 3],    [4, 5, 6],   [5, 7, 9]),\n",
    "    ([-1, -5, -3], [-4, -3, 0], [-5, -8, -3]),\n",
    "])\n",
    "def test_add_arrays(a, b, expect):\n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `parametrize` decorator takes two arguments:\n",
    "1. a string containing the names of the variables you want to pass in\n",
    "2. a list containing the values of the variables you want to pass in\n",
    "\n",
    "In this case, the test will be run twice with the following values:\n",
    "1. `a=[1, 2, 3]`, `b=[4, 5, 6]`, `expect=[5, 7, 9]`\n",
    "2. `a=[-1, -5, -3]`, `b=[-4, -3, 0]`, `expect=[-5, -8, -3]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 -- /home/matt/courses/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 2 items                                          \u001b[0m\n",
      "\n",
      "test_arrays.py::test_add_arrays[a0-b0-expect0] \u001b[32mPASSED\u001b[0m\u001b[36m [ 50%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays[a1-b1-expect1] \u001b[32mPASSED\u001b[0m\u001b[36m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 2 passed in 0.24s =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "- Add some more parameters to the `test_add_arrays` function.\n",
    "- Parametrise the `subtract_arrays` test function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failing correctly\n",
    "\n",
    "The interface is made up of the parameters a function expects and the values that is returns. If a user of a function knows these things then they are able to use it correctly. This is why we make sure to include this information in the docstring for all our functions.\n",
    "\n",
    "The other thing that is aprt of the interface of a function is any exceptions that are raised by it. If you need a refresher on exceptionns and error handling in Python, take a look at [the chapter on it in the Intermediate Python course](https://milliams.gitlab.io/intermediate_python/05%20Exceptions.html).\n",
    "\n",
    "To add explicit error handling to our function we need to do two things:\n",
    "1. add in a conditional `raise` statement\n",
    "2. document the fact that the function may raise something\n",
    "\n",
    "Let's add these to `arrays.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile arrays.py\n",
    "\n",
    "\"\"\"\n",
    "This module contains functions for manipulating and combining Python lists.\n",
    "\"\"\"\n",
    "\n",
    "def add_arrays(x, y):\n",
    "    \"\"\"\n",
    "    This function adds together each element of the two passed lists.\n",
    "\n",
    "    Args:\n",
    "        x (list): The first list to add\n",
    "        y (list): The second list to add\n",
    "\n",
    "    Returns:\n",
    "        list: the pairwise sums of ``x`` and ``y``.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the length of the lists ``x`` and ``y`` are different.\n",
    "\n",
    "    Examples:\n",
    "        >>> add_arrays([1, 4, 5], [4, 3, 5])\n",
    "        [5, 7, 10]\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"Both arrays must have the same length.\")\n",
    "    \n",
    "    z = []\n",
    "    for x_, y_ in zip(x, y):\n",
    "        z.append(x_ + y_)\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then test that the function correctly raises the exception when passes appropriate data.  Inside a pytest function we can require that a specific exception is raised by using [`pytest.raises`](https://docs.pytest.org/en/latest/reference.html#pytest-raises) in a `with` block. `pytest.raises` takes as an argument the type of an exception and if the block ends without that exception being rasie will fail the test.\n",
    "\n",
    "It may seem strange that we're testing and *requiring* that the function raises an error but it's important that if we've told our users that the code will produce a certain error in specific circumstances that it does as we promise.\n",
    "\n",
    "In our code we add a new test called `test_add_arrays_error` which does the check we require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "@pytest.mark.parametrize(\"a,b,expect\", [\n",
    "    ([1, 2, 3],    [4, 5, 6],   [5, 7, 9]),\n",
    "    ([-1, -5, -3], [-4, -3, 0], [-5, -8, -3]),\n",
    "])\n",
    "def test_add_arrays(a, b, expect):\n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect\n",
    "\n",
    "def test_add_arrays_error():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5]\n",
    "    with pytest.raises(ValueError):\n",
    "        output = add_arrays(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 -- /home/matt/courses/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 3 items                                          \u001b[0m\n",
      "\n",
      "test_arrays.py::test_add_arrays[a0-b0-expect0] \u001b[32mPASSED\u001b[0m\u001b[36m [ 33%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays[a1-b1-expect1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 66%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays_error \u001b[32mPASSED\u001b[0m\u001b[36m         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 3 passed in 0.46s =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Add a runtime test to the `subtract_arrays` function to check for unequal-length arguments.\n",
    "- Add a test for the exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doctests\n",
    "\n",
    "If you remember from when we were documenting out `add_arrays` function, we had a small section which gave the reader an example of how to sue the function:\n",
    "\n",
    "```\n",
    "Examples:\n",
    "    >>> add_arrays([1, 4, 5], [4, 3, 5])\n",
    "    [5, 7, 10]\n",
    "```\n",
    "\n",
    "Since this is valid Python code, we can ask pytest to run this code and check that the output we claimed would be returned is correct. If we pass `--doctest-modules` to the `pytest` command, it will search `.py` files for docstrings with example blocks and run them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 -- /home/matt/courses/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 4 items                                          \u001b[0m\u001b[1m\n",
      "\n",
      "arrays.py::arrays.add_arrays \u001b[32mPASSED\u001b[0m\u001b[36m                  [ 25%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays[a0-b0-expect0] \u001b[32mPASSED\u001b[0m\u001b[36m [ 50%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays[a1-b1-expect1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 75%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays_error \u001b[32mPASSED\u001b[0m\u001b[36m         [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m===================== warnings summary =====================\u001b[0m\n",
      "venv/lib64/python3.7/site-packages/jinja2/utils.py:485\n",
      "  /home/matt/courses/software_engineering_best_practices/venv/lib64/python3.7/site-packages/jinja2/utils.py:485: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "    from collections import MutableMapping\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/latest/warnings.html\n",
      "\u001b[33m\u001b[1m============== 4 passed, 1 warnings in 1.41s ===============\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v --doctest-modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here the test `arrays.py::arrays.add_arrays` which has passed. Ignore the warning about deprecation, this is from a third-party module which is leaking through.\n",
    "\n",
    "... doctests are really useful..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "See what happens when you break your doctest and run `pytest` again."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
