{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Testing is extremely important. Without testing, you cannot be sure that your code is doing what you think. Testing is an integral part of software development, and should be done *while* you are writing code, not after the code has been written.\n",
    "\n",
    "No doubt sofar, you have been manually checking that your code does the right thing. Perhaps you are tunning your code over a particular input file and making sure that you get a correct-looking plot out at the end. This is a start but how can you be sure that there's not a subtle bug that means that the output is incorrect? And if there *is* a problem, how will you be able to work out exactly which line of code it causing it?\n",
    "\n",
    "In order to be confident that our code it giving a correct output, a *test suite* is useful which provides a set of known inputs and checks that the code matches a set of known, expected outputs. To make it easier to locate where a bug is occuring, it's a good idea to make each individual test run over as small an amount of code as possible so that if *that* test fails, you know where to look for the problem. In Python this \"small unit of code\" is usually a function.\n",
    "\n",
    "Let's get started by making sure that our `add_arrays` function matches the outputs we expect.\n",
    "\n",
    "Since we are writing some tests for our `arrays` module, let's make a file called `test_arrays.py` which contains the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    if output == expect:\n",
    "        print(\"OK\")\n",
    "    else:\n",
    "        print(\"BROKEN\")\n",
    "\n",
    "test_add_arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script defines a function called `test_add_arrays` which defines some known input (`a` and `b`) and a known, matching output (`expect`). It passes them to the function `add_arrays` and compares the output to `expected`. It will either print `OK` or `BROKEN` depending on whether it's working or not. Finally, we explicitly call the test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# TODO Make this appear to the reader as `python test_arrays.py` and hide the \"In []\"\n",
    "\n",
    "%run test_arrays.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Break the test by changing either `a`, `b` or `expected` and rerun the test script. Make sure that it prints `BROKEN` in this case. Change it back to a working state once you've done this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... Change it to use an assert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect\n",
    "\n",
    "test_add_arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..now when we run we get nothing printed on success:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run test_arrays.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but on a failure we get an error printed like:\n",
    "\n",
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"test_arrays.py\", line 13, in <module>\n",
    "    test_add_arrays()\n",
    "  File \"test_arrays.py\", line 11, in test_add_arrays\n",
    "    assert output == expect\n",
    "AssertionError\n",
    "```\n",
    "\n",
    "which, if we had many test functions being run would tell us which one failed and on which line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...We want to automate the runnng of these tests and there is a tool called `pytest` which does this.\n",
    "\n",
    "Remove the call to `test_add_arrays()` on the last line of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run `pytest`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.1, py-1.8.0, pluggy-0.13.0\n",
      "rootdir: /home/matt/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_arrays.py \u001b[32m.\u001b[0m\u001b[36m                                                         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m============================== 1 passed in 1.50s ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=80 venv/bin/pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...If we break the test then we see...\n",
    "\n",
    "\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
