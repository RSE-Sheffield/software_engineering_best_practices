{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Testing is extremely important. Without testing, you cannot be sure that your code is doing what you think. Testing is an integral part of software development, and should be done *while* you are writing code, not after the code has been written.\n",
    "\n",
    "No doubt so far, you have been manually checking that your code does the right thing. Perhaps you are tunning your code over a particular input file and making sure that you get a correct-looking plot out at the end. This is a start but how can you be sure that there's not a subtle bug that means that the output is incorrect? And if there *is* a problem, how will you be able to work out exactly which line of code it causing it?\n",
    "\n",
    "In order to be confident that our code it giving a correct output, a *test suite* is useful which provides a set of known inputs and checks that the code matches a set of known, expected outputs. To make it easier to locate where a bug is occuring, it's a good idea to make each individual test run over as small an amount of code as possible so that if *that* test fails, you know where to look for the problem. In Python this \"small unit of code\" is usually a function.\n",
    "\n",
    "Let's get started by making sure that our `add_arrays` function matches the outputs we expect. As a reminder, this is what the file `arrays.py` looks like (though you will have a second function, `subtract_arrays` in yours):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile arrays.py\n",
    "\n",
    "\"\"\"\n",
    "This module contains functions for manipulating and combining Python lists.\n",
    "\"\"\"\n",
    "\n",
    "def add_arrays(x, y):\n",
    "    \"\"\"\n",
    "    This function adds together each element of the two passed lists.\n",
    "\n",
    "    Args:\n",
    "        x (list): The first list to add\n",
    "        y (list): The second list to add\n",
    "\n",
    "    Returns:\n",
    "        list: the pairwise sums of ``x`` and ``y``.\n",
    "\n",
    "    Examples:\n",
    "        >>> add_arrays([1, 4, 5], [4, 3, 5])\n",
    "        [5, 7, 10]\n",
    "    \"\"\"\n",
    "    z = []\n",
    "    for x_, y_ in zip(x, y):\n",
    "        z.append(x_ + y_)\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the name of the module we want to test is `arrays`, let's make a file called `test_arrays.py` which contains the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    if output == expect:\n",
    "        print(\"OK\")\n",
    "    else:\n",
    "        print(\"BROKEN\")\n",
    "\n",
    "test_add_arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script defines a function called `test_add_arrays` which defines some known input (`a` and `b`) and a known, matching output (`expect`). It passes them to the function `add_arrays` and compares the output to `expected`. It will either print `OK` or `BROKEN` depending on whether it's working or not. Finally, we explicitly call the test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run test_arrays.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Break the test by changing either `a`, `b` or `expected` and rerun the test script. Make sure that it prints `BROKEN` in this case. Change it back to a working state once you've done this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asserting\n",
    "\n",
    "The method used here works and runs the code correctly but it doesn't give very useful output. If we had five test functions in our file and three of them were failing we'd see something like:\n",
    "\n",
    "```\n",
    "OK\n",
    "BROKEN\n",
    "OK\n",
    "BROKEN\n",
    "BROKEN\n",
    "```\n",
    "\n",
    "We'd then have to cross-check back to our code to see which tests the `BROKEN`s referred to.\n",
    "\n",
    "To be able to automatically relate the output of the failing test to the place where your test failed, you can use an `assert` statement.\n",
    "\n",
    "An `assert` statement is followed by something which is either truthy of falsy. If it is truthy then nothing happens but if it is falsy then an exception is raised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 5 == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-05598cd61862>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 5 == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this `assert` statement in place of the `if`/`else` block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect\n",
    "\n",
    "test_add_arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we run the test script we get nothing printed on success:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run test_arrays.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but on a failure we get an error printed like:\n",
    "\n",
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"test_arrays.py\", line 13, in <module>\n",
    "    test_add_arrays()\n",
    "  File \"test_arrays.py\", line 11, in test_add_arrays\n",
    "    assert output == expect\n",
    "AssertionError\n",
    "```\n",
    "\n",
    "Which, like all exception messages gives us the location in the file at which the error occurred. This has the avantage that if we had many test functions being run it would tell us which one failed and on which line.\n",
    "\n",
    "The downside of using an `assert` like this is that as soon as one test fails, the whole script will halt and you'll only be informed of that one test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytest\n",
    "\n",
    "There's a few things that we've been doing so far that could be improved. Firstly, for every test function that we writem we then have to explicitly call it at the bottom of the test script like `test_add_arrays()`. This is error-prone as we might write a test function and forget to call it and then we would miss any errors it would catch.\n",
    "\n",
    "Secondly, we want nice, useful output from our test functions. Something better than the nothing/exception that a plain `assert` gives us. It would be nice to get a green `PASSED` for the good tests and a red `FAILED` for the bad ones alongside the name of the test in question.\n",
    "\n",
    "Finally, we want to make sure that all tests are run even if a test early in the process fails.\n",
    "\n",
    "Luckily, there is tool called *pytest* which can give us all of these things. It will work on our test script almost exactly as written with only one change needed.\n",
    "\n",
    "Remove the call to `test_add_arrays()` on the last line of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in the Terminal, run `pytest`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0\n",
      "rootdir: /home/matt/projects/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 10 items                                         \u001b[0m\n",
      "\n",
      "test_arrays.py \u001b[32m.\u001b[0m\u001b[36m                                     [ 10%]\u001b[0m\n",
      "test_books.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[36m                              [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 10 passed in 2.37s ====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytest will do two stages. First it will try to locate all the test function that it can find and then it will run each of them in turn, reporting the results.\n",
    "\n",
    "Here you can see that it's found that the file `test_arrays.py` contains a single test function. The green dot next tot he name of the file signifies the passing test. It then prints a summary at the end saying \"1 passed\".\n",
    "\n",
    "The way that `pytest` works is that it looks for files which are called `test_*.py` or `*_test.py` and look inside those for functions whose names begin with `test`.\n",
    "\n",
    "To see what it looks like when you have a failing test, let's deliberately break the test code by giving a wrong expected result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 999]  # Changed this to break the test\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run this test with `pytest` it should tell us that the test is indeed failing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0\n",
      "rootdir: /home/matt/projects/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 10 items                                         \u001b[0m\n",
      "\n",
      "test_arrays.py \u001b[31mF\u001b[0m\u001b[36m                                     [ 10%]\u001b[0m\n",
      "test_books.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[36m                              [100%]\u001b[0m\n",
      "\n",
      "========================= FAILURES =========================\n",
      "\u001b[31m\u001b[1m_____________________ test_add_arrays ______________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_add_arrays():\u001b[0m\n",
      "\u001b[1m        a = [1, 2, 3]\u001b[0m\n",
      "\u001b[1m        b = [4, 5, 6]\u001b[0m\n",
      "\u001b[1m        expect = [5, 7, 999]  # Changed this to break the test\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        output = add_arrays(a, b)\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m>       assert output == expect\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert [5, 7, 9] == [5, 7, 999]\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 2 diff: 9 != 999\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get the full diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_arrays.py\u001b[0m:11: AssertionError\n",
      "\u001b[31m\u001b[1m=============== 1 failed, 9 passed in 2.51s ================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from this is better than we saw with the plain `assert`. It's printing the full context of the contents of the test function with the line where the `assert` is failing being marked with a `>`. It then gives an expanded explanation of why the assert failed. Before we just got `AssertionError` but now it prints out the contents of `output` and `expect` and tells us that at index 2 of the list it's finding a `9` where we told it to expect a `999`.\n",
    "\n",
    "Before continuing, make sure that you change the file back to its previous contents by changing that `999` back to a `9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]  # Changed this back to 9\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Write a test which tests your `subtract_arrays` function from the previous chapter. Make sure it passes with a correct input/output and correctly fails if you break it on purpose. [<small>answer</small>](answer_subtract_test.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid repeating ourselves\n",
    "\n",
    "Having a single test for a function is already infinitely better than having none, but one test only gives you so much confidence. The real power of a test suite is being able to test your functions under lots of different conditions.\n",
    "\n",
    "Lets add a second test to check a different set of inputs and outputs to the `add_arrays` function and check that it passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays1():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect\n",
    "\n",
    "def test_add_arrays2():\n",
    "    a = [-1, -5, -3]\n",
    "    b = [-4, -3, 0]\n",
    "    expect = [-5, -8, -3]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run `pytest` we can optionally pass the `-v` flag which puts it in *verbose* mode. This will print out the tests being run, one per line which I find a more useful view most of the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/courses/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/projects/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 11 items                                         \u001b[0m\n",
      "\n",
      "test_arrays.py::test_add_arrays1 \u001b[32mPASSED\u001b[0m\u001b[36m              [  9%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays2 \u001b[32mPASSED\u001b[0m\u001b[36m              [ 18%]\u001b[0m\n",
      "test_books.py::test_word_counts[hat-33] \u001b[32mPASSED\u001b[0m\u001b[36m       [ 27%]\u001b[0m\n",
      "test_books.py::test_word_counts[freedom-71] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 36%]\u001b[0m\n",
      "test_books.py::test_word_counts[electricity-1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 45%]\u001b[0m\n",
      "test_books.py::test_word_counts[testing-3] \u001b[32mPASSED\u001b[0m\u001b[36m    [ 54%]\u001b[0m\n",
      "test_books.py::test_word_counts[Prince-1498] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 63%]\u001b[0m\n",
      "test_books.py::test_word_counts[internet-0] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 72%]\u001b[0m\n",
      "test_books.py::test_word_counts[Russia-71] \u001b[32mPASSED\u001b[0m\u001b[36m    [ 81%]\u001b[0m\n",
      "test_books.py::test_word_counts[Pierre-1260] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 90%]\u001b[0m\n",
      "test_books.py::test_word_counts[None-566311] \u001b[32mPASSED\u001b[0m\u001b[36m  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 11 passed in 3.11s ====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see both tests being run and passing. This will work well but we've had to repeat ourselves almost entirely in each test function. The only difference between the two functions is the inputs and outputs under test. Usually in this case in a function you would take these things as arguments and we can do the same thing here.\n",
    "\n",
    "The actual logic of the function is the following:\n",
    "\n",
    "```python\n",
    "def test_add_arrays(a, b, expect):\n",
    "    output = add_arrays(a, b)\n",
    "    assert output == expect\n",
    "```\n",
    "\n",
    "We then just need a way of passing the data we want to check into this function. For this, pytest provides a feature called *parametrisation*. We label our function with a *decoration* which allows pytest to run it mutliple times with different data.\n",
    "\n",
    "To use this feature we must import the `pytest` module and use the `pytest.mark.parametrize` decorator like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "@pytest.mark.parametrize(\"a, b, expect\", [\n",
    "    ([1, 2, 3],    [4, 5, 6],   [5, 7, 9]),\n",
    "    ([-1, -5, -3], [-4, -3, 0], [-5, -8, -3]),\n",
    "])\n",
    "def test_add_arrays(a, b, expect):\n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `parametrize` decorator takes two arguments:\n",
    "1. a string containing the names of the variables you want to pass in (`\"a, b, expect\"`)\n",
    "2. a list containing the values of the variables you want to pass in\n",
    "\n",
    "In this case, the test will be run twice. Once with each of the following values:\n",
    "1. `a=[1, 2, 3]`, `b=[4, 5, 6]`, `expect=[5, 7, 9]`\n",
    "2. `a=[-1, -5, -3]`, `b=[-4, -3, 0]`, `expect=[-5, -8, -3]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/courses/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/projects/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 11 items                                         \u001b[0m\n",
      "\n",
      "test_arrays.py::test_add_arrays[a0-b0-expect0] \u001b[32mPASSED\u001b[0m\u001b[36m [  9%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays[a1-b1-expect1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 18%]\u001b[0m\n",
      "test_books.py::test_word_counts[hat-33] \u001b[32mPASSED\u001b[0m\u001b[36m       [ 27%]\u001b[0m\n",
      "test_books.py::test_word_counts[freedom-71] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 36%]\u001b[0m\n",
      "test_books.py::test_word_counts[electricity-1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 45%]\u001b[0m\n",
      "test_books.py::test_word_counts[testing-3] \u001b[32mPASSED\u001b[0m\u001b[36m    [ 54%]\u001b[0m\n",
      "test_books.py::test_word_counts[Prince-1498] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 63%]\u001b[0m\n",
      "test_books.py::test_word_counts[internet-0] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 72%]\u001b[0m\n",
      "test_books.py::test_word_counts[Russia-71] \u001b[32mPASSED\u001b[0m\u001b[36m    [ 81%]\u001b[0m\n",
      "test_books.py::test_word_counts[Pierre-1260] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 90%]\u001b[0m\n",
      "test_books.py::test_word_counts[None-566311] \u001b[32mPASSED\u001b[0m\u001b[36m  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 11 passed in 2.67s ====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "- Add some more parameters to the `test_add_arrays` function.\n",
    "- Parametrise the `subtract_arrays` test function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failing correctly\n",
    "\n",
    "The interface of a function is made up of the *parameters* it expects and the values that it *returns*. If a user of a function knows these things then they are able to use it correctly. This is why we make sure to include this information in the docstring for all our functions.\n",
    "\n",
    "The other thing that is part of the interface of a function is any exceptions that are *raised* by it. If you need a refresher on exceptionns and error handling in Python, take a look at [the chapter on it in the Intermediate Python course](https://milliams.gitlab.io/intermediate_python/05%20Exceptions.html).\n",
    "\n",
    "To add explicit error handling to our function we need to do two things:\n",
    "1. add in a conditional `raise` statement:\n",
    "   ```python\n",
    "   if len(x) != len(y):\n",
    "       raise ValueError(\"Both arrays must have the same length.\")\n",
    "   ```\n",
    "2. document in the docstring the fact that the function may raise something:\n",
    "   ```\n",
    "   Raises:\n",
    "       ValueError: If the length of the lists ``x`` and ``y`` are different.\n",
    "   ```\n",
    "\n",
    "Let's add these to `arrays.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile arrays.py\n",
    "\n",
    "\"\"\"\n",
    "This module contains functions for manipulating and combining Python lists.\n",
    "\"\"\"\n",
    "\n",
    "def add_arrays(x, y):\n",
    "    \"\"\"\n",
    "    This function adds together each element of the two passed lists.\n",
    "\n",
    "    Args:\n",
    "        x (list): The first list to add\n",
    "        y (list): The second list to add\n",
    "\n",
    "    Returns:\n",
    "        list: the pairwise sums of ``x`` and ``y``.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the length of the lists ``x`` and ``y`` are different.\n",
    "\n",
    "    Examples:\n",
    "        >>> add_arrays([1, 4, 5], [4, 3, 5])\n",
    "        [5, 7, 10]\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"Both arrays must have the same length.\")\n",
    "    \n",
    "    z = []\n",
    "    for x_, y_ in zip(x, y):\n",
    "        z.append(x_ + y_)\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile arrays.py\n",
    "\n",
    "\"\"\"\n",
    "This module contains functions for manipulating and combining Python lists.\n",
    "\"\"\"\n",
    "\n",
    "def add_arrays(x, y):\n",
    "    \"\"\"\n",
    "    This function adds together each element of the two passed lists.\n",
    "\n",
    "    Args:\n",
    "        x (list): The first list to add\n",
    "        y (list): The second list to add\n",
    "\n",
    "    Returns:\n",
    "        list: the pairwise sums of ``x`` and ``y``.\n",
    "\n",
    "    Examples:\n",
    "        >>> add_arrays([1, 4, 5], [4, 3, 5])\n",
    "        [5, 7, 10]\n",
    "    \"\"\"\n",
    "\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"Both arrays must have the same length.\")\n",
    "\n",
    "    z = []\n",
    "    for x_, y_ in zip(x, y):\n",
    "        z.append(x_ + y_)\n",
    "\n",
    "    return z\n",
    "\n",
    "def subtract_arrays(x, y):\n",
    "    \"\"\"\n",
    "    This function subtracts from each other each element of the two passed lists.\n",
    "\n",
    "    Args:\n",
    "        x (list): The first list\n",
    "        y (list): The second list\n",
    "\n",
    "    Returns:\n",
    "        list: the pairwise difference of ``x`` and ``y``.\n",
    "\n",
    "    Examples:\n",
    "        >>> subtract_arrays([1, 4, 5], [4, 3, 5])\n",
    "        [-3, 1, 0]\n",
    "    \"\"\"\n",
    "\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"Both arrays must have the same length.\")\n",
    "\n",
    "    z = []\n",
    "    for x_, y_ in zip(x, y):\n",
    "        z.append(x_ - y_)\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then test that the function correctly raises the exception when passes appropriate data.  Inside a pytest function we can require that a specific exception is raised by using [`pytest.raises`](https://docs.pytest.org/en/latest/reference.html#pytest-raises) in a `with` block. `pytest.raises` takes as an argument the type of an exception and if the block ends without that exception habing been rasied, will fail the test.\n",
    "\n",
    "It may seem strange that we're testing-for and *requiring* that the function raises an error but it's important that if we've told our users that the code will produce a certain error in specific circumstances that it does indeed do as we promise.\n",
    "\n",
    "In our code we add a new test called `test_add_arrays_error` which does the check we require:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "@pytest.mark.parametrize(\"a, b, expect\", [\n",
    "    ([1, 2, 3],    [4, 5, 6],   [5, 7, 9]),\n",
    "    ([-1, -5, -3], [-4, -3, 0], [-5, -8, -3]),\n",
    "])\n",
    "def test_add_arrays(a, b, expect):\n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect\n",
    "\n",
    "def test_add_arrays_error():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5]\n",
    "    with pytest.raises(ValueError):\n",
    "        output = add_arrays(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/courses/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/projects/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 12 items                                         \u001b[0m\n",
      "\n",
      "test_arrays.py::test_add_arrays[a0-b0-expect0] \u001b[32mPASSED\u001b[0m\u001b[36m [  8%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays[a1-b1-expect1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 16%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays_error \u001b[32mPASSED\u001b[0m\u001b[36m         [ 25%]\u001b[0m\n",
      "test_books.py::test_word_counts[hat-33] \u001b[32mPASSED\u001b[0m\u001b[36m       [ 33%]\u001b[0m\n",
      "test_books.py::test_word_counts[freedom-71] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 41%]\u001b[0m\n",
      "test_books.py::test_word_counts[electricity-1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 50%]\u001b[0m\n",
      "test_books.py::test_word_counts[testing-3] \u001b[32mPASSED\u001b[0m\u001b[36m    [ 58%]\u001b[0m\n",
      "test_books.py::test_word_counts[Prince-1498] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 66%]\u001b[0m\n",
      "test_books.py::test_word_counts[internet-0] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 75%]\u001b[0m\n",
      "test_books.py::test_word_counts[Russia-71] \u001b[32mPASSED\u001b[0m\u001b[36m    [ 83%]\u001b[0m\n",
      "test_books.py::test_word_counts[Pierre-1260] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 91%]\u001b[0m\n",
      "test_books.py::test_word_counts[None-566311] \u001b[32mPASSED\u001b[0m\u001b[36m  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 12 passed in 2.23s ====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "- Add a runtime test to the `subtract_arrays` function to check for unequal-length arguments.\n",
    "- Add a test for the exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doctests\n",
    "\n",
    "If you remember from when we were documenting out `add_arrays` function, we had a small section which gave the reader an example of how to use the function:\n",
    "\n",
    "```\n",
    "Examples:\n",
    "    >>> add_arrays([1, 4, 5], [4, 3, 5])\n",
    "    [5, 7, 10]\n",
    "```\n",
    "\n",
    "Since this is valid Python code, we can ask pytest to run this code and check that the output we claimed would be returned is correct. If we pass `--doctest-modules` to the `pytest` command, it will search `.py` files for docstrings with example blocks and run them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false,
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/courses/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/projects/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 14 items                                         \u001b[0m\n",
      "\n",
      "arrays.py::arrays.add_arrays \u001b[32mPASSED\u001b[0m\u001b[36m                  [  7%]\u001b[0m\n",
      "arrays.py::arrays.subtract_arrays \u001b[32mPASSED\u001b[0m\u001b[36m             [ 14%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays[a0-b0-expect0] \u001b[32mPASSED\u001b[0m\u001b[36m [ 21%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays[a1-b1-expect1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 28%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays_error \u001b[32mPASSED\u001b[0m\u001b[36m         [ 35%]\u001b[0m\n",
      "test_books.py::test_word_counts[hat-33] \u001b[32mPASSED\u001b[0m\u001b[36m       [ 42%]\u001b[0m\n",
      "test_books.py::test_word_counts[freedom-71] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 50%]\u001b[0m\n",
      "test_books.py::test_word_counts[electricity-1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 57%]\u001b[0m\n",
      "test_books.py::test_word_counts[testing-3] \u001b[32mPASSED\u001b[0m\u001b[36m    [ 64%]\u001b[0m\n",
      "test_books.py::test_word_counts[Prince-1498] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 71%]\u001b[0m\n",
      "test_books.py::test_word_counts[internet-0] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 78%]\u001b[0m\n",
      "test_books.py::test_word_counts[Russia-71] \u001b[32mPASSED\u001b[0m\u001b[36m    [ 85%]\u001b[0m\n",
      "test_books.py::test_word_counts[Pierre-1260] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 92%]\u001b[0m\n",
      "test_books.py::test_word_counts[None-566311] \u001b[32mPASSED\u001b[0m\u001b[36m  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 14 passed in 2.29s ====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v --doctest-modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here the `arrays.py::arrays.add_arrays` test which has passed. Ignore the warning about deprecation, this is from a third-party module which is leaking through.\n",
    "\n",
    "Doctests are a really valuable thing to have in your test suite as they ensure that any examples that you are giving work as expected. It's not uncommon for the code to change and for the documentation to be left behind and begin able to automatically check all your examples avoids this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "See what happens when you break your doctest and run `pytest` again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running specific tests\n",
    "\n",
    "As you increase the number of tests you will come across situations where you only want to run a particular test. TO do this, you follow pass the name of the test, as printed by `pytest -v` as an argument to `pytest`. So, if we want to run all tests in `test_arrays.py` we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\r\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/courses/software_engineering_best_practices/venv/bin/python3\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /home/matt/projects/courses/software_engineering_best_practices\r\n",
      "plugins: nbval-0.9.3\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 3 items                                          \u001b[0m\r\n",
      "\r\n",
      "test_arrays.py::test_add_arrays[a0-b0-expect0] \u001b[32mPASSED\u001b[0m\u001b[36m [ 33%]\u001b[0m\r\n",
      "test_arrays.py::test_add_arrays[a1-b1-expect1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 66%]\u001b[0m\r\n",
      "test_arrays.py::test_add_arrays_error \u001b[32mPASSED\u001b[0m\u001b[36m         [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m\u001b[1m==================== 3 passed in 0.01s =====================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v test_arrays.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if we want to specifically run the `test_add_arrays` test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\r\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/courses/software_engineering_best_practices/venv/bin/python3\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /home/matt/projects/courses/software_engineering_best_practices\r\n",
      "plugins: nbval-0.9.3\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 2 items                                          \u001b[0m\r\n",
      "\r\n",
      "test_arrays.py::test_add_arrays[a0-b0-expect0] \u001b[32mPASSED\u001b[0m\u001b[36m [ 50%]\u001b[0m\r\n",
      "test_arrays.py::test_add_arrays[a1-b1-expect1] \u001b[32mPASSED\u001b[0m\u001b[36m [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m\u001b[1m==================== 2 passed in 0.01s =====================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v test_arrays.py::test_add_arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if we want to run one test specifically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\r\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/courses/software_engineering_best_practices/venv/bin/python3\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /home/matt/projects/courses/software_engineering_best_practices\r\n",
      "plugins: nbval-0.9.3\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 1 item                                           \u001b[0m\r\n",
      "\r\n",
      "test_arrays.py::test_add_arrays[a0-b0-expect0] \u001b[32mPASSED\u001b[0m\u001b[36m [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m\u001b[1m==================== 1 passed in 0.01s =====================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v test_arrays.py::test_add_arrays[a0-b0-expect0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the output of `pytest -h` for more options. For example, you can tell `pytest` to only run the tests that failed on the last run with `pytest --last-failed`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data for tests\n",
    "\n",
    "As we saw above when using parametrisation, it's often useful to split your test function into two parts:\n",
    "1. The data to be tested\n",
    "2. The code to do the test\n",
    "\n",
    "This is because we had a situation where we had one test function and multiple examples to test. The opposite situation also happens where we have multiple test functions, all of which want the same input data.\n",
    "\n",
    "The name that pytest uses for \"data which is provided to test functions\" is *fixture* since it *fixes* a set of data against which to test.\n",
    "\n",
    "We'll start with the example of the `add_arrays` function to demonstrate the syntax but soon we'll need to use a example which demonstates the benefits more.\n",
    "\n",
    "To make things clearer, we'll trim down the test file back to the basics. Just one test for `add_arrays` and one for `subtract_arrays`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays, subtract_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect\n",
    "\n",
    "def test_subtract_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [-3, -3, -3]\n",
    "    \n",
    "    output = subtract_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these tests use the same values for `a` and `b`. In this case it's not a big problem that we're repeating ourselves here but in a more complex test suite the data were testing over may be very complicated or slow to create.\n",
    "\n",
    "To create our fixture we define a function which is decorated with the `pytest.fixture` decorator. Apart from that, all the function needs do is return the data we want to provide to our tests:\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def pair_of_lists():\n",
    "    return [1, 2, 3], [4, 5, 6]\n",
    "```\n",
    "\n",
    "To make the test functions make use of the fixture, we use the name of the fixture (`pair_of_lists`) as a parameter of the test function, similar to how we did with parametrisation:\n",
    "\n",
    "```python\n",
    "def test_add_arrays(pair_of_lists):\n",
    "    ...\n",
    "```\n",
    "\n",
    "The data is now available inside the function using that name and we can use it however we wish:\n",
    "\n",
    "```python\n",
    "def test_add_arrays(pair_of_lists):\n",
    "    a, b = pair_of_lists\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "from arrays import add_arrays, subtract_arrays\n",
    "\n",
    "@pytest.fixture\n",
    "def pair_of_lists():\n",
    "    return [1, 2, 3], [4, 5, 6]\n",
    "\n",
    "def test_add_arrays(pair_of_lists):\n",
    "    a, b = pair_of_lists\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect\n",
    "\n",
    "def test_subtract_arrays(pair_of_lists):\n",
    "    a, b = pair_of_lists\n",
    "    expect = [-3, -3, -3]\n",
    "    \n",
    "    output = subtract_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the test suite, pytest will automatically run the `pair_of_lists` function for each test and pass in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/courses/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/projects/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 11 items                                         \u001b[0m\n",
      "\n",
      "test_arrays.py::test_add_arrays \u001b[32mPASSED\u001b[0m\u001b[36m               [  9%]\u001b[0m\n",
      "test_arrays.py::test_subtract_arrays \u001b[32mPASSED\u001b[0m\u001b[36m          [ 18%]\u001b[0m\n",
      "test_books.py::test_word_counts[hat-33] \u001b[32mPASSED\u001b[0m\u001b[36m       [ 27%]\u001b[0m\n",
      "test_books.py::test_word_counts[freedom-71] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 36%]\u001b[0m\n",
      "test_books.py::test_word_counts[electricity-1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 45%]\u001b[0m\n",
      "test_books.py::test_word_counts[testing-3] \u001b[32mPASSED\u001b[0m\u001b[36m    [ 54%]\u001b[0m\n",
      "test_books.py::test_word_counts[Prince-1498] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 63%]\u001b[0m\n",
      "test_books.py::test_word_counts[internet-0] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 72%]\u001b[0m\n",
      "test_books.py::test_word_counts[Russia-71] \u001b[32mPASSED\u001b[0m\u001b[36m    [ 81%]\u001b[0m\n",
      "test_books.py::test_word_counts[Pierre-1260] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 90%]\u001b[0m\n",
      "test_books.py::test_word_counts[None-566311] \u001b[32mPASSED\u001b[0m\u001b[36m  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 11 passed in 2.72s ====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be hard to see the benefit of fixtures with this rather contrived example so lets take a look at a more sensible one where using a fixture makes sense.\n",
    "\n",
    "Make a new file called `test_books.py` which contains the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting books.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile books.py\n",
    "\n",
    "def word_count(text: str, word: str='') -> int:\n",
    "    \"\"\"\n",
    "    Count the number of occurences of ``word`` in a string.\n",
    "    If ``word`` is not set, count all words.\n",
    "    \"\"\"\n",
    "    if word:\n",
    "        count = 0\n",
    "        for text_word in text.split():\n",
    "            if text_word == word:\n",
    "                count += 1\n",
    "        return count\n",
    "    else:\n",
    "        return len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this function we want a corpus of text to test it over. For the purposes of this example and to simulate a complex data input, we will download the contents of a long book from Project Gutenberg. Our test function uses [`urllib.request`](https://docs.python.org/3/library/urllib.request.html) to download the text, converts it to a string and passes that to the `word_count` function. At first we will simply check that the word \"hat\" appears 33 times in the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_books.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_books.py\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "from books import word_count\n",
    "\n",
    "def test_word_counts():\n",
    "    url = \"https://www.gutenberg.org/files/2600/2600-0.txt\"\n",
    "    book_text = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    assert word_count(book_text, \"hat\") == 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/courses/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/projects/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 1 item                                           \u001b[0m\n",
      "\n",
      "test_books.py::test_word_counts \u001b[32mPASSED\u001b[0m\u001b[36m               [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 1 passed in 1.91s =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v test_books.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...took ~2 seconds...\n",
    "\n",
    "...not too bad, but if we want to test multiple..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_books.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_books.py\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import pytest\n",
    "\n",
    "from books import word_count\n",
    "\n",
    "@pytest.mark.parametrize('word,count',  [\n",
    "    ('hat', 33),\n",
    "    ('freedom', 71),\n",
    "    ('electricity', 1),\n",
    "    ('testing', 3),\n",
    "    ('Prince', 1498),\n",
    "    ('internet', 0),\n",
    "    ('Russia', 71),\n",
    "    ('Pierre', 1260),\n",
    "    (None, 566311),\n",
    "])\n",
    "def test_word_counts(word, count):\n",
    "    url = \"https://www.gutenberg.org/files/2600/2600-0.txt\"\n",
    "    book_text = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    assert word_count(book_text, word) == count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/courses/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/projects/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 9 items                                          \u001b[0m\n",
      "\n",
      "test_books.py::test_word_counts[hat-33] \u001b[32mPASSED\u001b[0m\u001b[36m       [ 11%]\u001b[0m\n",
      "test_books.py::test_word_counts[freedom-71] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 22%]\u001b[0m\n",
      "test_books.py::test_word_counts[electricity-1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 33%]\u001b[0m\n",
      "test_books.py::test_word_counts[testing-3] \u001b[32mPASSED\u001b[0m\u001b[36m    [ 44%]\u001b[0m\n",
      "test_books.py::test_word_counts[Prince-1498] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 55%]\u001b[0m\n",
      "test_books.py::test_word_counts[internet-0] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 66%]\u001b[0m\n",
      "test_books.py::test_word_counts[Russia-71] \u001b[32mPASSED\u001b[0m\u001b[36m    [ 77%]\u001b[0m\n",
      "test_books.py::test_word_counts[Pierre-1260] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 88%]\u001b[0m\n",
      "test_books.py::test_word_counts[None-566311] \u001b[32mPASSED\u001b[0m\u001b[36m  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 9 passed in 15.91s ====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v test_books.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...It took nine times as long...\n",
    "\n",
    "...move the slow setup into a fixture..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_books.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_books.py\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import pytest\n",
    "\n",
    "from books import word_count\n",
    "\n",
    "@pytest.fixture()\n",
    "def long_book():\n",
    "    url = \"https://www.gutenberg.org/files/2600/2600-0.txt\"\n",
    "    book_text = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    return book_text\n",
    "\n",
    "@pytest.mark.parametrize('word,count',  [\n",
    "    ('hat', 33),\n",
    "    ('freedom', 71),\n",
    "    ('electricity', 1),\n",
    "    ('testing', 3),\n",
    "    ('Prince', 1498),\n",
    "    ('internet', 0),\n",
    "    ('Russia', 71),\n",
    "    ('Pierre', 1260),\n",
    "    (None, 566311),\n",
    "])\n",
    "def test_word_counts(long_book, word, count):\n",
    "    assert word_count(long_book, word) == count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/courses/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/projects/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 9 items                                          \u001b[0m\n",
      "\n",
      "test_books.py::test_word_counts[hat-33] \u001b[32mPASSED\u001b[0m\u001b[36m       [ 11%]\u001b[0m\n",
      "test_books.py::test_word_counts[freedom-71] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 22%]\u001b[0m\n",
      "test_books.py::test_word_counts[electricity-1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 33%]\u001b[0m\n",
      "test_books.py::test_word_counts[testing-3] \u001b[32mPASSED\u001b[0m\u001b[36m    [ 44%]\u001b[0m\n",
      "test_books.py::test_word_counts[Prince-1498] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 55%]\u001b[0m\n",
      "test_books.py::test_word_counts[internet-0] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 66%]\u001b[0m\n",
      "test_books.py::test_word_counts[Russia-71] \u001b[32mPASSED\u001b[0m\u001b[36m    [ 77%]\u001b[0m\n",
      "test_books.py::test_word_counts[Pierre-1260] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 88%]\u001b[0m\n",
      "test_books.py::test_word_counts[None-566311] \u001b[32mPASSED\u001b[0m\u001b[36m  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 9 passed in 17.90s ====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v test_books.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...need to use `scope=\"module\"` as an argument to `pytest.fixture`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_books.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_books.py\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import pytest\n",
    "\n",
    "from books import word_count\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def long_book():\n",
    "    url = \"https://www.gutenberg.org/files/2600/2600-0.txt\"\n",
    "    book_text = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    return book_text\n",
    "\n",
    "@pytest.mark.parametrize('word,count',  [\n",
    "    ('hat', 33),\n",
    "    ('freedom', 71),\n",
    "    ('electricity', 1),\n",
    "    ('testing', 3),\n",
    "    ('Prince', 1498),\n",
    "    ('internet', 0),\n",
    "    ('Russia', 71),\n",
    "    ('Pierre', 1260),\n",
    "    (None, 566311),\n",
    "])\n",
    "def test_word_counts(long_book, word, count):\n",
    "    assert word_count(long_book, word) == count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/courses/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/projects/courses/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 9 items                                          \u001b[0m\n",
      "\n",
      "test_books.py::test_word_counts[hat-33] \u001b[32mPASSED\u001b[0m\u001b[36m       [ 11%]\u001b[0m\n",
      "test_books.py::test_word_counts[freedom-71] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 22%]\u001b[0m\n",
      "test_books.py::test_word_counts[electricity-1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 33%]\u001b[0m\n",
      "test_books.py::test_word_counts[testing-3] \u001b[32mPASSED\u001b[0m\u001b[36m    [ 44%]\u001b[0m\n",
      "test_books.py::test_word_counts[Prince-1498] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 55%]\u001b[0m\n",
      "test_books.py::test_word_counts[internet-0] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 66%]\u001b[0m\n",
      "test_books.py::test_word_counts[Russia-71] \u001b[32mPASSED\u001b[0m\u001b[36m    [ 77%]\u001b[0m\n",
      "test_books.py::test_word_counts[Pierre-1260] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 88%]\u001b[0m\n",
      "test_books.py::test_word_counts[None-566311] \u001b[32mPASSED\u001b[0m\u001b[36m  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 9 passed in 2.55s =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v test_books.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...now fast..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Add some more parameters to the test and check that it doesn't take any longer to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to commit\n",
    "\n",
    "... CI ..."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
