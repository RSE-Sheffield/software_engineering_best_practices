{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Testing is extremely important. Without testing, you cannot be sure that your code is doing what you think. Testing is an integral part of software development, and should be done *while* you are writing code, not after the code has been written.\n",
    "\n",
    "No doubt so far, you have been manually checking that your code does the right thing. Perhaps you are tunning your code over a particular input file and making sure that you get a correct-looking plot out at the end. This is a start but how can you be sure that there's not a subtle bug that means that the output is incorrect? And if there *is* a problem, how will you be able to work out exactly which line of code it causing it?\n",
    "\n",
    "In order to be confident that our code it giving a correct output, a *test suite* is useful which provides a set of known inputs and checks that the code matches a set of known, expected outputs. To make it easier to locate where a bug is occuring, it's a good idea to make each individual test run over as small an amount of code as possible so that if *that* test fails, you know where to look for the problem. In Python this \"small unit of code\" is usually a function.\n",
    "\n",
    "Let's get started by making sure that our `add_arrays` function matches the outputs we expect. As a reminder, this is what the file `arrays.py` looks like (though you will have a second function, `subtract_arrays` in yours):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile arrays.py\n",
    "\n",
    "\"\"\"\n",
    "This module contains functions for manipulating and combining Python lists.\n",
    "\"\"\"\n",
    "\n",
    "def add_arrays(x, y):\n",
    "    \"\"\"\n",
    "    This function adds together each element of the two passed lists.\n",
    "\n",
    "    Args:\n",
    "        x (list): The first list to add\n",
    "        y (list): The second list to add\n",
    "\n",
    "    Returns:\n",
    "        list: the pairwise sums of ``x`` and ``y``.\n",
    "\n",
    "    Examples:\n",
    "        >>> add_arrays([1, 4, 5], [4, 3, 5])\n",
    "        [5, 7, 10]\n",
    "    \"\"\"\n",
    "    z = []\n",
    "    for x_, y_ in zip(x, y):\n",
    "        z.append(x_ + y_)\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the name of the module we want to test is `arrays`, let's make a file called `test_arrays.py` which contains the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    if output == expect:\n",
    "        print(\"OK\")\n",
    "    else:\n",
    "        print(\"BROKEN\")\n",
    "\n",
    "test_add_arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script defines a function called `test_add_arrays` which defines some known input (`a` and `b`) and a known, matching output (`expect`). It passes them to the function `add_arrays` and compares the output to `expected`. It will either print `OK` or `BROKEN` depending on whether it's working or not. Finally, we explicitly call the test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run test_arrays.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Break the test by changing either `a`, `b` or `expected` and rerun the test script. Make sure that it prints `BROKEN` in this case. Change it back to a working state once you've done this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asserting\n",
    "\n",
    "The method used here works and runs the code correctly but it doesn't give very useful output. If we had five test functions in our file and three of them were failing we'd see something like:\n",
    "\n",
    "```\n",
    "OK\n",
    "BROKEN\n",
    "OK\n",
    "BROKEN\n",
    "BROKEN\n",
    "```\n",
    "\n",
    "We'd then have to cross-check back to our code to see which tests the `BROKEN`s referred to.\n",
    "\n",
    "To be able to automatically relate the output of the failing test to the place where your test failed, you can use an `assert` statement.\n",
    "\n",
    "An `assert` statement is followed by something which is either truthy of falsy. If it is truthy then nothing happens but if it is falsy then an exception is raised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 5 == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-05598cd61862>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 5 == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this `assert` statement in place of the `if`/`else` block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect\n",
    "\n",
    "test_add_arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we run the test script we get nothing printed on success:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run test_arrays.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but on a failure we get an error printed like:\n",
    "\n",
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"test_arrays.py\", line 13, in <module>\n",
    "    test_add_arrays()\n",
    "  File \"test_arrays.py\", line 11, in test_add_arrays\n",
    "    assert output == expect\n",
    "AssertionError\n",
    "```\n",
    "\n",
    "Which, like all exception messages gives us the location in the file at which the error occurred. This has the avantage that if we had many test functions being run it would tell us which one failed and on which line.\n",
    "\n",
    "The downside of using an `assert` like this is that as soon as one test fails, the whole script will halt and you'll only be informed of that one test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytest\n",
    "\n",
    "There's a few things that we've been doing so far that could be improved. Firstly, for every test function that we writem we then have to explicitly call it at the bottom of the test script like `test_add_arrays()`. This is error-prone as we might write a test function and forget to call it and then we would miss any errors it would catch.\n",
    "\n",
    "Secondly, we want nice, useful output from our test functions. Something better than the nothing/exception that a plain `assert` gives us. It would be nice to get a green `PASSED` for the good tests and a red `FAILED` for the bad ones alongside the name of the test in question.\n",
    "\n",
    "Finally, we want to make sure that all tests are run even if a test early in the process fails.\n",
    "\n",
    "Luckily, there is tool called *pytest* which can give us all of these things. It will work on our test script almost exactly as written with only one change needed.\n",
    "\n",
    "Remove the call to `test_add_arrays()` on the last line of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in the Terminal, run `pytest`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\r\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0\r\n",
      "rootdir: /home/matt/projects/software_engineering_best_practices\r\n",
      "plugins: nbval-0.9.3\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 1 item                                           \u001b[0m\r\n",
      "\r\n",
      "test_arrays.py \u001b[32m.\u001b[0m\u001b[36m                                     [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m\u001b[1m==================== 1 passed in 0.02s =====================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytest will do two stages. First it will try to locate all the test function that it can find and then it will run each of them in turn, reporting the results.\n",
    "\n",
    "Here you can see that it's found that the file `test_arrays.py` contains a single test function. The green dot next tot he name of the file signifies the passing test. It then prints a summary at the end saying \"1 passed\".\n",
    "\n",
    "The way that `pytest` works is that it looks for files which are called `test_*.py` or `*_test.py` and look inside those for functions whose names begin with `test`.\n",
    "\n",
    "To see what it looks like when you have a failing test, let's deliberately break the test code by giving a wrong expected result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 999]  # Changed this to break the test\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run this test with `pytest` it should tell us that the test is indeed failing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0\n",
      "rootdir: /home/matt/projects/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 1 item                                           \u001b[0m\n",
      "\n",
      "test_arrays.py \u001b[31mF\u001b[0m\u001b[36m                                     [100%]\u001b[0m\n",
      "\n",
      "========================= FAILURES =========================\n",
      "\u001b[31m\u001b[1m_____________________ test_add_arrays ______________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_add_arrays():\u001b[0m\n",
      "\u001b[1m        a = [1, 2, 3]\u001b[0m\n",
      "\u001b[1m        b = [4, 5, 6]\u001b[0m\n",
      "\u001b[1m        expect = [5, 7, 999]  # Changed this to break the test\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        output = add_arrays(a, b)\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m>       assert output == expect\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert [5, 7, 9] == [5, 7, 999]\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 2 diff: 9 != 999\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get the full diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_arrays.py\u001b[0m:11: AssertionError\n",
      "\u001b[31m\u001b[1m==================== 1 failed in 0.19s =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from this is better than we saw with the plain `assert`. It's printing the full context of the contents of the test function with the line where the `assert` is failing being marked with a `>`. It then gives an expanded explanation of why the assert failed. Before we just got `AssertionError` but now it prints out the contents of `output` and `expect` and tells us that at index 2 of the list it's finding a `9` where we told it to expect a `999`.\n",
    "\n",
    "Before continuing, make sure that you change the file back to its previous contents by changing that `999` back to a `9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "remove_cell",
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]  # Changed this back to 9\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Write a test which tests your `subtract_arrays` function from the previous chapter. Make sure it passes with a correct input/output and correctly fails if you break it on purpose. [<small>answer</small>](answer_subtract_test.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid repeating ourselves\n",
    "\n",
    "Having a single test for a function is already infinitely better than having none, but one test only gives you so much confidence. The real power of a test suite is being able to test your functions under lots of different conditions.\n",
    "\n",
    "Lets add a second test to check a different set of inputs and outputs to the `add_arrays` function and check that it passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "def test_add_arrays1():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5, 6]\n",
    "    expect = [5, 7, 9]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect\n",
    "\n",
    "def test_add_arrays2():\n",
    "    a = [-1, -5, -3]\n",
    "    b = [-4, -3, 0]\n",
    "    expect = [-5, -8, -3]\n",
    "    \n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run `pytest` we can optionally pass the `-v` flag which puts it in *verbose* mode. This will print out the tests being run, one per line which I find a more useful view most of the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/projects/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 2 items                                          \u001b[0m\n",
      "\n",
      "test_arrays.py::test_add_arrays1 \u001b[32mPASSED\u001b[0m\u001b[36m              [ 50%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays2 \u001b[32mPASSED\u001b[0m\u001b[36m              [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 2 passed in 0.03s =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see both tests being run and passing. This will work well but we've had to repeat ourselves almost entirely in each test function. The only difference between the two functions is the inputs and outputs under test. Usually in this case in a function you would take these things as arguments and we can do the same thing here.\n",
    "\n",
    "The actual logic of the function is the following:\n",
    "\n",
    "```python\n",
    "def test_add_arrays(a, b, expect):\n",
    "    output = add_arrays(a, b)\n",
    "    assert output == expect\n",
    "```\n",
    "\n",
    "We then just need a way of passing the data we want to check into this function. For this, pytest provides a feature called *parametrisation*. We label our function with a *decoration* which allows pytest to run it mutliple times with different data.\n",
    "\n",
    "To use this feature we must import the `pytest` module and use the `pytest.mark.parametrize` decorator like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "@pytest.mark.parametrize(\"a, b, expect\", [\n",
    "    ([1, 2, 3],    [4, 5, 6],   [5, 7, 9]),\n",
    "    ([-1, -5, -3], [-4, -3, 0], [-5, -8, -3]),\n",
    "])\n",
    "def test_add_arrays(a, b, expect):\n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `parametrize` decorator takes two arguments:\n",
    "1. a string containing the names of the variables you want to pass in (`\"a, b, expect\"`)\n",
    "2. a list containing the values of the variables you want to pass in\n",
    "\n",
    "In this case, the test will be run twice. Once with each of the following values:\n",
    "1. `a=[1, 2, 3]`, `b=[4, 5, 6]`, `expect=[5, 7, 9]`\n",
    "2. `a=[-1, -5, -3]`, `b=[-4, -3, 0]`, `expect=[-5, -8, -3]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/projects/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 2 items                                          \u001b[0m\n",
      "\n",
      "test_arrays.py::test_add_arrays[a0-b0-expect0] \u001b[32mPASSED\u001b[0m\u001b[36m [ 50%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays[a1-b1-expect1] \u001b[32mPASSED\u001b[0m\u001b[36m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 2 passed in 0.04s =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "- Add some more parameters to the `test_add_arrays` function.\n",
    "- Parametrise the `subtract_arrays` test function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failing correctly\n",
    "\n",
    "The interface of a function is made up of the *parameters* it expects and the values that it *returns*. If a user of a function knows these things then they are able to use it correctly. This is why we make sure to include this information in the docstring for all our functions.\n",
    "\n",
    "The other thing that is part of the interface of a function is any exceptions that are *raised* by it. If you need a refresher on exceptionns and error handling in Python, take a look at [the chapter on it in the Intermediate Python course](https://milliams.gitlab.io/intermediate_python/05%20Exceptions.html).\n",
    "\n",
    "To add explicit error handling to our function we need to do two things:\n",
    "1. add in a conditional `raise` statement:\n",
    "   ```python\n",
    "   if len(x) != len(y):\n",
    "       raise ValueError(\"Both arrays must have the same length.\")\n",
    "   ```\n",
    "2. document in the docstring the fact that the function may raise something:\n",
    "   ```\n",
    "   Raises:\n",
    "       ValueError: If the length of the lists ``x`` and ``y`` are different.\n",
    "   ```\n",
    "\n",
    "Let's add these to `arrays.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile arrays.py\n",
    "\n",
    "\"\"\"\n",
    "This module contains functions for manipulating and combining Python lists.\n",
    "\"\"\"\n",
    "\n",
    "def add_arrays(x, y):\n",
    "    \"\"\"\n",
    "    This function adds together each element of the two passed lists.\n",
    "\n",
    "    Args:\n",
    "        x (list): The first list to add\n",
    "        y (list): The second list to add\n",
    "\n",
    "    Returns:\n",
    "        list: the pairwise sums of ``x`` and ``y``.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the length of the lists ``x`` and ``y`` are different.\n",
    "\n",
    "    Examples:\n",
    "        >>> add_arrays([1, 4, 5], [4, 3, 5])\n",
    "        [5, 7, 10]\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"Both arrays must have the same length.\")\n",
    "    \n",
    "    z = []\n",
    "    for x_, y_ in zip(x, y):\n",
    "        z.append(x_ + y_)\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then test that the function correctly raises the exception when passes appropriate data.  Inside a pytest function we can require that a specific exception is raised by using [`pytest.raises`](https://docs.pytest.org/en/latest/reference.html#pytest-raises) in a `with` block. `pytest.raises` takes as an argument the type of an exception and if the block ends without that exception habing been rasied, will fail the test.\n",
    "\n",
    "It may seem strange that we're testing-for and *requiring* that the function raises an error but it's important that if we've told our users that the code will produce a certain error in specific circumstances that it does indeed do as we promise.\n",
    "\n",
    "In our code we add a new test called `test_add_arrays_error` which does the check we require:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_arrays.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_arrays.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "from arrays import add_arrays\n",
    "\n",
    "@pytest.mark.parametrize(\"a, b, expect\", [\n",
    "    ([1, 2, 3],    [4, 5, 6],   [5, 7, 9]),\n",
    "    ([-1, -5, -3], [-4, -3, 0], [-5, -8, -3]),\n",
    "])\n",
    "def test_add_arrays(a, b, expect):\n",
    "    output = add_arrays(a, b)\n",
    "    \n",
    "    assert output == expect\n",
    "\n",
    "def test_add_arrays_error():\n",
    "    a = [1, 2, 3]\n",
    "    b = [4, 5]\n",
    "    with pytest.raises(ValueError):\n",
    "        output = add_arrays(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/projects/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 3 items                                          \u001b[0m\n",
      "\n",
      "test_arrays.py::test_add_arrays[a0-b0-expect0] \u001b[32mPASSED\u001b[0m\u001b[36m [ 33%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays[a1-b1-expect1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 66%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays_error \u001b[32mPASSED\u001b[0m\u001b[36m         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 3 passed in 0.04s =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "- Add a runtime test to the `subtract_arrays` function to check for unequal-length arguments.\n",
    "- Add a test for the exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doctests\n",
    "\n",
    "If you remember from when we were documenting out `add_arrays` function, we had a small section which gave the reader an example of how to use the function:\n",
    "\n",
    "```\n",
    "Examples:\n",
    "    >>> add_arrays([1, 4, 5], [4, 3, 5])\n",
    "    [5, 7, 10]\n",
    "```\n",
    "\n",
    "Since this is valid Python code, we can ask pytest to run this code and check that the output we claimed would be returned is correct. If we pass `--doctest-modules` to the `pytest` command, it will search `.py` files for docstrings with example blocks and run them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false,
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/projects/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 4 items                                          \u001b[0m\n",
      "\n",
      "arrays.py::arrays.add_arrays \u001b[32mPASSED\u001b[0m\u001b[36m                  [ 25%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays[a0-b0-expect0] \u001b[32mPASSED\u001b[0m\u001b[36m [ 50%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays[a1-b1-expect1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 75%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays_error \u001b[32mPASSED\u001b[0m\u001b[36m         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 4 passed in 0.20s =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v --doctest-modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here the `arrays.py::arrays.add_arrays` test which has passed. Ignore the warning about deprecation, this is from a third-party module which is leaking through.\n",
    "\n",
    "Doctests are a really valuable thing to have in your test suite as they ensure that any examples that you are giving work as expected. It's not uncommon for the code to change and for the documentation to be left behind and begin able to automatically check all your examples avoids this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Exercise\n",
    "\n",
    "See what happens when you break your doctest and run `pytest` again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running specific tests\n",
    "\n",
    "As you increase the number of tests you will come across situations where you only want to run a particular test. TO do this, you follow pass the name of the test, as printed by `pytest -v` as an argument to `pytest`. So, if we want to run all tests in `test_arrays.py` we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\r\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/software_engineering_best_practices/venv/bin/python3\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /home/matt/projects/software_engineering_best_practices\r\n",
      "plugins: nbval-0.9.3\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 3 items                                          \u001b[0m\r\n",
      "\r\n",
      "test_arrays.py::test_add_arrays[a0-b0-expect0] \u001b[32mPASSED\u001b[0m\u001b[36m [ 33%]\u001b[0m\r\n",
      "test_arrays.py::test_add_arrays[a1-b1-expect1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 66%]\u001b[0m\r\n",
      "test_arrays.py::test_add_arrays_error \u001b[32mPASSED\u001b[0m\u001b[36m         [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m\u001b[1m==================== 3 passed in 0.02s =====================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v test_arrays.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if we want to specifically run the `test_add_arrays` test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/software_engineering_best_practices/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/matt/projects/software_engineering_best_practices\n",
      "plugins: nbval-0.9.3\n",
      "collected 2 items                                          \u001b[0m\n",
      "\n",
      "test_arrays.py::test_add_arrays[a0-b0-expect0] \u001b[32mPASSED\u001b[0m\u001b[36m [ 50%]\u001b[0m\n",
      "test_arrays.py::test_add_arrays[a1-b1-expect1] \u001b[32mPASSED\u001b[0m\u001b[36m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m==================== 2 passed in 0.02s =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v test_arrays.py::test_add_arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if we want to run one test specifically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=================== test session starts ====================\u001b[0m\r\n",
      "platform linux -- Python 3.7.3, pytest-5.2.2, py-1.8.0, pluggy-0.13.0 -- /home/matt/projects/software_engineering_best_practices/venv/bin/python3\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /home/matt/projects/software_engineering_best_practices\r\n",
      "plugins: nbval-0.9.3\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 1 item                                           \u001b[0m\r\n",
      "\r\n",
      "test_arrays.py::test_add_arrays[a0-b0-expect0] \u001b[32mPASSED\u001b[0m\u001b[36m [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m\u001b[1m==================== 1 passed in 0.02s =====================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!COLUMNS=60 venv/bin/pytest -v test_arrays.py::test_add_arrays[a0-b0-expect0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the output of `pytest -h` for more options. For example, you can tell `pytest` to only run the tests that failed on the last run with `pytest --last-failed`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing during development\n",
    "\n",
    "When developing code I recommend starting your day by running the test suite. This gives you confidence in the code before you start playing around with it. Then after each set of changes, run the tests again to make sure that you have not broken anything.\n",
    "\n",
    "As you add new feature and functions to your code, add tests for them straight away. Doing this while the code logic is fresh in your mind will make the test writing much easier. Even better is to write the tests for the code directly alongside the new function. This gives you a strutured way of checking your function while you work on it. You're likely doing manual testing of your code as you write it anyway so why not automate it so that once you're finished you've improved your test suite for free!\n",
    "\n",
    "Some people go a step further and follow a process called *test-driven development* where the first thing you do is write the docstring for the function, describing what the function will take as arguments and what it will return. Once you know that, you can write a test which will, in principle, pass if the function does as desribed. Only then will they go ahead and write the body of the function and once the test passes, their job is done.\n",
    "\n",
    "These techniques are part of a spectrum so find the place on it which makes you the most productive in the long-term. When I started programming I never wrote tests but as I've progressed I've found that they make writing correct code much easier and I couldn't write software without them. You will likely find a progression towards a more structured approach over time.\n",
    "\n",
    "## Automated tests\n",
    "\n",
    "We will not cover it in this course but once you have a good test suite and if your source code is hosted on a public Git website then you can make it so that the tests of your code are automatically run on every change. As long as you commit your `test_*.py` to Git files then with a few lines of config you are able to make it run your tests for you. This is especially useful once you are collaborating with other people as the atuomated tests will give you confidence that the new feature your colleage is adding has not broken the code you wrote last week for example."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
